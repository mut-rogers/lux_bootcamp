{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Statement \n",
    "_____\n",
    "- In this project, we shall be extracting data from Jumia (www.jumia.co.ke) an e-Commerce website. \n",
    "\n",
    "- We shall be scrapping the website to access products with discounts currently. \n",
    "\n",
    "- The data will be moved to a Postgres database housed at Aiven - (https://aiven.io/) \n",
    "\n",
    "#### Key libraries for this projects include;\n",
    "___\n",
    "\n",
    "1. Beautiful Soup - `pip install beautifulsoup4`\n",
    "\n",
    "2. Pandas - `pip install pandas`\n",
    "\n",
    "3. requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1: Setting up the project \n",
    "\n",
    "- Importing the libraries,\n",
    "\n",
    "- Setting project variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libaries \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd \n",
    "import lxml\n",
    "import requests \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.jumia.co.ke/{}/?page={}#catalog-listing\" # This is the BASE_URL that will be used in this project\n",
    "\n",
    "# This list will hold the product categories we shall scrape\n",
    "PRODUCT_CATEGORIES = [\n",
    "    \"electronics\",\n",
    "    \"phones-tablets\",\n",
    "    # \"category-fashion-by-jumia\",\n",
    "    # \"home-office\",\n",
    "    # \"health-beauty\",\n",
    "    # \"home-office-appliances\",\n",
    "    # \"computing\",\n",
    "    # \"baby-products\",\n",
    "    # \"sporting-goods\"\n",
    "]\n",
    "\n",
    "MAX_PAGE_COUNT = 3 # Sets the number of pages to scrape for every product category. Max = 50\n",
    "\n",
    "# To make sure that we are sending requests as user agennts for all our HTTP requests.\n",
    "# The default user agent using python requests in Python\n",
    "PAGE_HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper() -> list:\n",
    "    \"\"\" \n",
    "    This function scrapes the project URL to find products, thier prices, and discounts prices\n",
    "\n",
    "    Returns:\n",
    "        all_products (list): A list of dictionaries containing products that have been scrapped.\n",
    "    \"\"\" \n",
    "\n",
    "    all_products = [] # The scraped products will be added here as a list of dictionaries\n",
    "\n",
    "    current_page_num = 1 # Holds the value for the current page being scrapped \n",
    "\n",
    "    # Looping through the product categories of interest\n",
    "    for product_category in PRODUCT_CATEGORIES:\n",
    "\n",
    "        # Make sure we don't try to access pages that don't exist\n",
    "        while current_page_num <= MAX_PAGE_COUNT: \n",
    "\n",
    "            response = requests.get(BASE_URL.format(product_category, current_page_num), headers=PAGE_HEADERS) \n",
    "            print(\"Showing you The first every page number\", current_page_num)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'lxml') # Create a soup \n",
    "\n",
    "            products_wrapper = soup.find_all(\"article\", {\"class\": \"prd _fb col c-prd\"})  # Find all the HTML tags wrapping each product\n",
    "            \n",
    "            # Loop and access each wrapper to access specific information for earch product\n",
    "            for product in products_wrapper:\n",
    "                product_name = product.find(\"h3\", {\"class\": \"name\"}).text # Access the product name \n",
    "\n",
    "                current_price = product.find(\"div\", {\"class\": \"prc\"}).text # Access the current price \n",
    " \n",
    "                try: # Accounting for products that may not have old price\n",
    "                    old_price = product.find(\"div\", {\"class\": \"old\"}).text\n",
    "                except:\n",
    "                    old_price = \"0\" \n",
    "\n",
    "                # Create a dictionary for this product and append to the list all_products\n",
    "                current_product_details = {\n",
    "                    \"product_name\": product_name,\n",
    "                    \"category\": product_category,\n",
    "                    \"current_price\": current_price,\n",
    "                    \"old_price\": old_price\n",
    "                } \n",
    "\n",
    "                all_products.append(current_product_details)\n",
    "            \n",
    "            current_page_num = current_page_num + 1 # Increment this to move to the next page\n",
    "\n",
    "            # We want the scrapper to pause for 4 seconds before making another request\n",
    "            print(\"Scrapper going to sleep...\")\n",
    "            time.sleep(4)\n",
    "            \n",
    "        # Reset the page counter when done with each category\n",
    "        current_page_num = 1\n",
    "\n",
    "    return all_products "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Storage \n",
    "___ \n",
    "This stage involves storing the scrapped data to the database \n",
    "\n",
    "Below are the implementation details;\n",
    "\n",
    "- Move the data to a Pandas Dataframe. \n",
    "\n",
    "- Perform some data cleaning tasks, e.g., transformations \n",
    "\n",
    "- Set up the database\n",
    "\n",
    "- Use Pandas to move the cleaned data to our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
